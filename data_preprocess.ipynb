{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我的csv文件里的'note'列里面有需要进行截断的文本，['CHIEF COMPLAINT',‘PAST SURGICAL HISTORY’,'REVIEW OF SYSTEMS','EMERGENCY DEPARTMENT COURSE','GYNECOLOGIC HISTORY','PAST MEDICAL HISTORY','HISTORY of PRESENT ILLNESS','FAMILY HISTORY/SOCIAL HISTORY']，当这些节标题出现的时候我需要对note里面的文本进行截断，以这些节标题作为key，以这些节标题后对应的文本为values,保存到字典中，最后以'encounter_id'列里对应的id存到json中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 读取CSV文件\n",
    "data = pd.read_csv('./MEDIQA-Chat-Training-ValidationSets-Feb-10-2023/TaskB/TaskB-TrainingSet.csv')\n",
    "\n",
    "# 需要截断的节标题\n",
    "section_headers = ['CHIEF COMPLAINT','PAST SURGICAL HISTORY','REVIEW OF SYSTEMS','RESULTS','ASSESSMENT AND PLAN','EMERGENCY DEPARTMENT COURSE','GYNECOLOGIC HISTORY','PAST MEDICAL HISTORY','HISTORY OF PRESENT ILLNESS','FAMILY HISTORY/SOCIAL HISTORY','PHYSICAL EXAMINATION','VITALS REVIEWED']\n",
    "\n",
    "# 用来组成输出json的id列\n",
    "id_col = 'encounter_id'\n",
    "\n",
    "def split_text(text, keywords):\n",
    "    keyword_locations = {}\n",
    "    for keyword in keywords:\n",
    "        index = text.find(keyword)\n",
    "        if index != -1:\n",
    "            keyword_locations[keyword]=index\n",
    "        else:\n",
    "            keyword_locations[keyword]=len(text)\n",
    "    \n",
    "    keyword_locations = sorted(keyword_locations.items(), key=lambda d:d[1])\n",
    "    result = []\n",
    "    for i in range(len(keyword_locations)):\n",
    "        start = keyword_locations[i][1] + len(keyword_locations[i][0])\n",
    "        if i == len(keyword_locations)-1:\n",
    "            end = len(text)+1 \n",
    "        else:\n",
    "            end = keyword_locations[i+1][1]\n",
    "        if i == 0 and start != 0:\n",
    "            start = len(keyword_locations[i][0])+1\n",
    "           \n",
    "        result.append(text[start:end].strip())\n",
    "        \n",
    "    return keyword_locations,result\n",
    "\n",
    "\n",
    "# 初始化输出json对象\n",
    "output = []\n",
    "\n",
    "# 遍历每个encounter_id\n",
    "for id_value in data[id_col].unique():\n",
    "    # 创建该encounter_id对应的输出字典\n",
    "    encounter_dict = {}\n",
    "    \n",
    "    # 获取该encounter_id对应的行\n",
    "    encounter_rows = data[data[id_col] == id_value]\n",
    "    \n",
    "    # 遍历该encounter_id对应的所有行\n",
    "    for index, row in encounter_rows.iterrows():\n",
    "        # section_header = row['section_header']\n",
    "        note = row['note']\n",
    "        section_header=[]\n",
    "        for sec in section_headers:\n",
    "            if sec in note:\n",
    "                # print(sec)\n",
    "                section_header.append(sec)\n",
    "        # 判断该行是否包含需要截断的节标题\n",
    "        # if section_header in section_headers:\n",
    "        #     # 截断文本\n",
    "        #     note = note.split(section_header, 1)[-1]\n",
    "            \n",
    "        #     # 将截断后的文本以对应的节标题为key，添加到输出字典中\n",
    "        #     if section_header in encounter_dict:\n",
    "        #         encounter_dict[section_header] += note\n",
    "        #     else:\n",
    "        #         encounter_dict[section_header] = note\n",
    "        section_dict,res = split_text(keywords= section_header,text=note)\n",
    "        # print(res)\n",
    "        for i,sec in enumerate(section_dict):\n",
    "            # print(i)\n",
    "            encounter_dict[section_dict[i][0]]= res[i]\n",
    "    \n",
    "    # 将该encounter_id和对应的字典添加到输出json对象中\n",
    "    output.append({id_col: id_value, 'sections': encounter_dict})\n",
    "\n",
    "# 将输出json对象保存到文件中\n",
    "with open('output.json', 'w') as f:\n",
    "    json.dump(output, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试进行句子截断，使用chunking方式"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_preprocessing(x):\n",
    "    # replacing special line-break characters in text by whitespace\n",
    "    return x.replace('\\n', ' ').replace('\\t', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% helper functions for chunking method\n",
    "def compute_break_index(utterance_lengths, start_idx, end_idx, overlap):\n",
    "    # 计算需要切割的句子index值（看看在哪切割）\n",
    "    assert overlap >= 0.0 and overlap <= 1.0\n",
    "    num_utterances = end_idx - start_idx\n",
    "    fragment_length = sum(utterance_lengths[start_idx:end_idx])\n",
    "    running_length = 0\n",
    "    idx = end_idx\n",
    "    # 窗口往回推，推到满足设置overlap重叠部分\n",
    "    for i in range(num_utterances):\n",
    "        running_overlap = running_length / float(fragment_length)\n",
    "        if running_overlap >= overlap:\n",
    "            return idx\n",
    "        idx -= 1\n",
    "        running_length += utterance_lengths[idx]\n",
    "    return start_idx + 1\n",
    "\n",
    "def chunk_conversation(x, header_length, fragment_length, fragment_overlap):\n",
    "    num_utterances = len(x)\n",
    "    utterance_lengths = []\n",
    "    for i in range(len(x)):\n",
    "        v = len(x[i][\"utterance\"].split())\n",
    "        # 计算每句对话的长度\n",
    "        utterance_lengths.append(v)\n",
    "    total_length = sum(utterance_lengths)\n",
    "    header_utterances = [] # 头对话\n",
    "    fragments = [] # 切割的对话窗口\n",
    "    if total_length <= header_length + fragment_length:\n",
    "        # 如果全部对话很短，直接就不用分了\n",
    "        header_utterances = x\n",
    "    else:\n",
    "        idx = 0\n",
    "        h_length = 0\n",
    "        # getting the header. 获得头对话\n",
    "        while h_length + utterance_lengths[idx] <= header_length:\n",
    "            header_utterances.append(x[idx])\n",
    "            h_length += utterance_lengths[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # TODO: check that I'm not off by one.\n",
    "        # getting the fragments. 获得分割后的对话窗口\n",
    "        while idx < num_utterances:\n",
    "            f_length = 0\n",
    "            start_idx = idx\n",
    "            fragment_utterances = []\n",
    "            while idx < num_utterances and f_length + utterance_lengths[idx] <= fragment_length:\n",
    "                fragment_utterances.append(x[idx])\n",
    "                f_length += utterance_lengths[idx]\n",
    "                idx += 1\n",
    "            if len(fragment_utterances) >= 1:\n",
    "                # 把切割的对话保存下来\n",
    "                fragments.append(fragment_utterances)\n",
    "\n",
    "            # prevent complete overlap. 以防分割对话重叠\n",
    "            if idx < num_utterances:\n",
    "                break_idx = compute_break_index(utterance_lengths, start_idx, idx, fragment_overlap)\n",
    "                assert break_idx != start_idx\n",
    "                idx = break_idx\n",
    "\n",
    "    return header_utterances, fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_conversation_fragments(header_utterances, fragments, \n",
    "utterance_separator_str=' ', header_fragment_separator_str='...', continuation_str='...'):\n",
    "    header_strs = []\n",
    "    for u in header_utterances:\n",
    "        s = format_line(u)\n",
    "        header_strs.append(s)\n",
    "\n",
    "    fragment_strs_lst = []\n",
    "    for f in fragments:\n",
    "        f_strs = []\n",
    "        for u in f:\n",
    "            s = format_line(u)\n",
    "            f_strs.append(s)\n",
    "        fragment_strs_lst.append(f_strs)\n",
    "\n",
    "    out_strs = []\n",
    "    header_str = utterance_separator_str.join(header_strs)\n",
    "    if len(fragment_strs_lst) == 0:\n",
    "        out_strs.append(header_str)\n",
    "    else:\n",
    "        num_fragments = len(fragment_strs_lst)\n",
    "        assert num_fragments >= 1\n",
    "\n",
    "        for i, f_strs in enumerate(fragment_strs_lst):\n",
    "            fragment_str = utterance_separator_str.join(f_strs)\n",
    "\n",
    "            # first fragment\n",
    "            if i == 0:\n",
    "                out_s = utterance_separator_str.join([header_str, fragment_str])\n",
    "                if len(fragment_strs_lst) > 1:\n",
    "                    out_s = utterance_separator_str.join([out_s, continuation_str])\n",
    "            # inner fragments\n",
    "            elif i < num_fragments - 1:\n",
    "                out_s = utterance_separator_str.join([header_str, header_fragment_separator_str, fragment_str, continuation_str])\n",
    "            # last fragment\n",
    "            else:\n",
    "                out_s = utterance_separator_str.join([header_str, header_fragment_separator_str, fragment_str])\n",
    "\n",
    "            out_strs.append(out_s)\n",
    "    return out_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% main APIs\n",
    "def read_data(filename, cid=\"cid\", stringify=True):\n",
    "    \"\"\"Create a DataFrame from .jsonl|.json|.pckl file.\n",
    "\n",
    "    Required Parameters\n",
    "    -------------------\n",
    "    filename: str, path to input data file\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    cid: str (default 'cid'), name of the column used for conversation identifier\n",
    "    stringify: bool (default True), whether to force the <cid> column to be strings\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: pandas.DataFrame object, data loaded in as a pandas dataframe\n",
    "    \"\"\"\n",
    "    if filename.endswith('.jsonl'):\n",
    "        df = []\n",
    "        with jsonlines.open(filename, mode='r') as reader:\n",
    "            for j in reader:\n",
    "                df.append(j)\n",
    "        df = pd.DataFrame(df)\n",
    "    elif filename.endswith('.json'):\n",
    "        df = pd.read_json(filename)\n",
    "    elif filename.endswith('.pckl'):\n",
    "        df = pd.read_pickle(filename)\n",
    "    else:\n",
    "        raise TypeError('Unrecognized file extension, supported are .jsonl|.json|.pckl')\n",
    "\n",
    "    if stringify:\n",
    "        df[cid] = df[cid].astype(str)\n",
    "    df.sort_values(cid, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_file(df, folder, savefile, meta_cols=['cid', 'sid'], src_col=None, tgt_col=None):\n",
    "    \"\"\"Saving dataframe object to .meta|.source|.target files.\n",
    "\n",
    "    Required Parameters\n",
    "    -------------------\n",
    "    filename: str, path to input data file\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    cid: str (default 'cid'), name of the column used for conversation identifier\n",
    "    stringify: bool (default True), whether to force the <cid> column to be strings\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    df: pandas.DataFrame object, data loaded in as a pandas dataframe\n",
    "    \"\"\"\n",
    "    if os.path.exists(folder):\n",
    "        print(f\"Warning! {folder}/ already exists, files with identical names will be overwritten\")\n",
    "    else:\n",
    "        os.makedirs(folder)\n",
    "    savefile = os.path.join(folder, savefile)\n",
    "    df[meta_cols].to_csv(savefile+'.meta', sep='\\t', index=True, header=True)\n",
    "    print('saving meta file to {}'.format(savefile+'.meta'))\n",
    "    if src_col is not None:\n",
    "        with open(savefile+'.source', 'w') as writer:\n",
    "            writer.write('\\n'.join(df[src_col]))\n",
    "        print('saving source file to {}'.format(savefile+'.source'))\n",
    "    if tgt_col is not None:\n",
    "        with open(savefile+'.target', 'w') as writer:\n",
    "            writer.write('\\n'.join(df[tgt_col]))\n",
    "        print('saving target file to {}'.format(savefile+'.target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_data_stage1(filename, exp='', savefolder='../experiments/',\n",
    "                               save=True, process_fn=None,\n",
    "                               header_len=128, body_len=384, body_overlap=0.333,\n",
    "                               **kwds):\n",
    "    \"\"\"Generate necessary data files for Multistage training (Chunking method) - stage 1.\n",
    "\n",
    "    Required Parameters\n",
    "    -------------------\n",
    "    filename: str, path to input data file\n",
    "\n",
    "    Keyword Parameters\n",
    "    ------------------\n",
    "    exp: str (default ''), name of the experiment, used to create a separate folder under <savefolder> for storing all files related to current experiment.\n",
    "    savefolder: str (default '../experiments/'), path to folder storing all experiments.\n",
    "    save: bool (default True), whether to save the .meta|.source|.target files.\n",
    "    process_fn: function handle (default None), additional data preprocessing functions. The function must take in and return both a pandas.DataFrame object.\n",
    "    header_len: int (default 128), header component length in unit of words.\n",
    "    body_len: int (default 384), body component length in unit of words.\n",
    "    body_overlap: float (default 0.333), a floating value between 0 and 1, the percentage of overlap in unit of words between the body components of two adjacent chunks.\n",
    "    **kwds: additional keyword parameters supported by process_fn().\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    dfout: pandas.DataFrame object, dataframe object containing data after preprocessing for stage 1 chunking method.\n",
    "    \"\"\"\n",
    "    df = read_data(filename)\n",
    "    \n",
    "    if process_fn is not None:\n",
    "        df = process_fn(df, **kwds)\n",
    "        \n",
    "    snippets = []\n",
    "    for (i, row) in df.iterrows():\n",
    "        x = row['utterances']\n",
    "        header_utterances, fragments = chunk_conversation(x, header_len, body_len, body_overlap)\n",
    "        out_strs = serialize_conversation_fragments(header_utterances, fragments, utterance_separator_str=' ', header_fragment_separator_str='...', continuation_str='...')\n",
    "        snippets.append(out_strs)\n",
    "        # print(max([len(x.split()) for x in out_strs]))\n",
    "    df['chunks'] = snippets\n",
    "    dfout = df[['cid', 'sid', 'chunks', 'summary']].explode('chunks', ignore_index=True)\n",
    "    dfout['summary'] = dfout['summary'].apply(bart_preprocessing)\n",
    "    dfout['chunks'] = dfout['chunks'].apply(bart_preprocessing)\n",
    "\n",
    "    if save:\n",
    "        mode = get_mode(filename)\n",
    "        folder = os.path.join(savefolder, exp)\n",
    "        save_file(\n",
    "            dfout, \n",
    "            folder,\n",
    "            f'{mode}_stage1', \n",
    "            meta_cols=['cid', 'sid'], \n",
    "            src_col='chunks', \n",
    "            tgt_col='summary',\n",
    "        )\n",
    "        save_file(\n",
    "            dfout, \n",
    "            folder,\n",
    "            f'{mode}_stagex', \n",
    "            meta_cols=['cid', 'sid'], \n",
    "            src_col='chunks', \n",
    "            tgt_col='summary',\n",
    "        )\n",
    "\n",
    "    return dfout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dialogue_sents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dialogue) \u001b[39m>\u001b[39m MAX_LEN:\n\u001b[1;32m     21\u001b[0m     \u001b[39m# Truncate dialogue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     retained_sents \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(MAX_LEN \u001b[39m*\u001b[39m RETAIN_PERCENT)\n\u001b[0;32m---> 23\u001b[0m     truncated_sents \u001b[39m=\u001b[39m dialogue_sents[:retained_sents]\n\u001b[1;32m     24\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(dialogue_sents) \u001b[39m-\u001b[39m retained_sents):\n\u001b[1;32m     25\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m retained_sents \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dialogue_sents' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Set maximum length of dialogue and percentage to retain\n",
    "MAX_LEN = 30\n",
    "RETAIN_PERCENT = 0.2\n",
    "\n",
    "# Sample dialogue and summary\n",
    "dialogue = \"Person A: Hi, how are you doing? Person B: I'm good, thanks for asking. Person A: What have you been up to lately? Person B: Not much, just working and hanging out with friends. Person A: That sounds fun. Person B: Yeah, it is. What about you? Person A: I've been pretty busy with work. Person B: That's too bad.\"\n",
    "summary = \"Person A and Person B discussed what they had been up to lately. Person B had been working and hanging out with friends, while Person A had been busy with work.\"\n",
    "\n",
    "# Tokenize dialogue into sentences\n",
    "# dialogue_sents = sent_tokenize(dialogue)\n",
    "embedding_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Check if dialogue length exceeds maximum length\n",
    "if len(dialogue) > MAX_LEN:\n",
    "    # Truncate dialogue\n",
    "    retained_sents = int(MAX_LEN * RETAIN_PERCENT)\n",
    "    truncated_sents = dialogue_sents[:retained_sents]\n",
    "    for i in range(1, len(dialogue_sents) - retained_sents):\n",
    "        if i % retained_sents == 0:\n",
    "            truncated_sents += dialogue_sents[i:i+retained_sents]\n",
    "    truncated_sents += dialogue_sents[-retained_sents:]\n",
    "else:\n",
    "    truncated_sents = dialogue_sents\n",
    "print(truncated_sents)\n",
    "# Calculate semantic similarity between truncated dialogue and each summary sentence\n",
    "summary_sents = sent_tokenize(summary)\n",
    "similarity_scores = []\n",
    "for truncated_sent in truncated_sents:\n",
    "    truncated_vec = np.mean(embedding_model.encode(truncated_sent), axis=0).reshape(1,-1)\n",
    "    for summary_sent in summary_sents:\n",
    "        summary_vec = np.mean(embedding_model.encode(summary_sent), axis=0).reshape(1,-1)\n",
    "        similarity_score = cosine_similarity(truncated_vec, summary_vec)[0][0]\n",
    "        similarity_scores.append(similarity_score)\n",
    "\n",
    "# Find the most similar summary sentence for each truncated dialogue sentence\n",
    "most_similar_indices = []\n",
    "for i in range(len(truncated_sents)):\n",
    "    start_index = i * len(summary_sents)\n",
    "    end_index = start_index + len(summary_sents)\n",
    "    most_similar_indices.append(np.argmax(similarity_scores[start_index:end_index]) + i * len(summary_sents))\n",
    "\n",
    "# Get corresponding summary sentences for each truncated dialogue sentence\n",
    "corresponding_summary_sents = [summary_sents[i % len(summary_sents)] for i in most_similar_indices]\n",
    "\n",
    "# Join truncated dialogue sentences and corresponding summary sentences\n",
    "truncated_dialogue = ' '.join(truncated_sents)\n",
    "corresponding_summary = ' '.join(corresponding_summary_sents)\n",
    "\n",
    "# Print results\n",
    "print(\"Original dialogue:\\n\", dialogue)\n",
    "print(\"Truncated dialogue:\\n\", truncated_dialogue)\n",
    "print(\"Original summary:\\n\", summary)\n",
    "print(\"Corresponding summary:\\n\", corresponding_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os \n",
    "# 设定文本的最大长度\n",
    "MAX_LEN = 512\n",
    "# 设定每个截断后的部分都有20%的重合\n",
    "OVERLAP = 0.2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "model = SentenceTransformer('sentence-transformers/nli-bert-large-cls-pooling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (583759428.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[27], line 57\u001b[0;36m\u001b[0m\n\u001b[0;31m    def split_then_alig\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def length_split(text, max_len, overlap):\n",
    "    '''\n",
    "    将文本截断成多个子部分，每个部分的长度为max_len，相邻两部分有overlap的重叠部分\n",
    "    '''\n",
    "    text_len = len(text)\n",
    "    # 如果文本长度小于等于max_len，不需要截断\n",
    "    if text_len <= max_len:\n",
    "        return [text]\n",
    "    # 计算需要截断成几个部分\n",
    "    step = int(max_len * (1 - overlap))\n",
    "    num_parts = int(np.ceil((text_len - max_len) / step)) + 1\n",
    "    # 对每个部分进行截断\n",
    "    parts = []\n",
    "    for i in range(num_parts):\n",
    "        start = i * step\n",
    "        end = min(start + max_len, text_len)\n",
    "        parts.append(text[start:end])\n",
    "    return parts\n",
    "\n",
    "def turn_split(text):\n",
    "    # 根据回车进行了分句\n",
    "    dialogue = text.split('\\n')\n",
    "    # \n",
    "    dialogue = [dia.strip() for dia in dialogue]\n",
    "    return dialogue\n",
    "def group_dialogues(dialogue_list,n=2):\n",
    "    num_groups = len(dialogue_list) // n\n",
    "    \n",
    "    # 将列表中的元素按照n个为一组进行分组\n",
    "    groups = [dialogue_list[i:i+n] for i in range(0, num_groups*n, n)]\n",
    "    \n",
    "    # 如果列表中的元素不能被n整除，则将剩余的元素作为一组\n",
    "    if len(dialogue_list) % n != 0:\n",
    "        groups.append(dialogue_list[num_groups*n:])\n",
    "    # 将每一个组中的字符串拼接成一个字符串\n",
    "    groups = [' '.join(group) for group in groups]\n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "def utterences_summary_alignment(dialogue_parts,summary):\n",
    "    '''以句子为基准对齐摘要'''\n",
    "    # dialogue_parts = split_text(dialogue, MAX_LEN, OVERLAP)\n",
    "    dialogue_embeddings = model.encode(dialogue_parts)\n",
    "    summary = nltk.sent_tokenize(summary)\n",
    "    summary_embeddings = model.encode(summary)\n",
    "    # print(dialogue_embeddings)\n",
    "    # print(summary_embeddings)\n",
    "    cosine_scores = util.pytorch_cos_sim(dialogue_embeddings, summary_embeddings)\n",
    "    # print(cosine_scores)\n",
    "    summary_sentences  = []\n",
    "    for i, dialog_sentence in enumerate(dialogue_parts):\n",
    "        max_index = np.argmax(cosine_scores[i])\n",
    "        summary_sentence = summary[max_index]\n",
    "        summary_sentences.append(summary_sentence)\n",
    "        \n",
    "        print(\"Dialog sentence: \", dialog_sentence)\n",
    "        print(\"Summary sentence: \", summary_sentence)\n",
    "        \n",
    "    return summary_sentences\n",
    "\n",
    "def align_dialogue_with_summary(dialogue, summary_sentences):\n",
    "    '''以摘要为基准对句子\n",
    "    这里导入的dialogue应该先切分更好,是一个list，list里面是一个个string'''\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    # dialogue = turn_split(dialogue)\n",
    "    dialogue_vectors = model.encode(dialogue, convert_to_tensor=True)\n",
    "    summary_vectors = model.encode(summary_sentences, convert_to_tensor=True)\n",
    "    similarity_matrix = cosine_similarity(summary_vectors, dialogue_vectors)\n",
    "    summary_sentences_to_snippets=[]\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        match_indices = np.where(similarity_matrix[i] >= 0.7)[0]\n",
    "        if match_indices:\n",
    "\n",
    "            segments = group_dialogue(match_indices, dialogue)\n",
    "        else:\n",
    "            segments= ''\n",
    "        summary_sentences_to_snippets[i] = segments\n",
    "    return summary_sentences_to_snippets\n",
    "\n",
    "def group_dialogue(indices, dialogue):\n",
    "    group = []\n",
    "    segment = []\n",
    "    for i, idx in enumerate(indices):\n",
    "        segment.append(dialogue[idx])\n",
    "    # 返回的是一个句子string\n",
    "    group.append(' '.join(segment))\n",
    "    return group\n",
    "\n",
    "\n",
    "\n",
    "def split_then_alignment(dialogue, summary, split=\"length\", MAX_LEN=512, OVERLAP=0.2):\n",
    "    if split == \"length\":\n",
    "        dialogue_parts = length_split(dialogue,MAX_LEN,OVERLAP)\n",
    "    elif split == \"turn\":\n",
    "        dialogue_parts = turn_split(dialogue)\n",
    "        dialogue_parts = group_dialogues(dialogue_parts)\n",
    "    summary_sentences = utterences_summary_alignment(dialogue_parts,summary=summary)\n",
    "    # TODO 应该写一下按utterences来对齐还是按summary的sentence来对齐\n",
    "    return dialogue_parts, summary_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Doctor: What brings you back into the clinic today, miss?',\n",
       "  'Patient: I came in for a refill of my blood pressure medicine.',\n",
       "  'Doctor: It looks like Doctor Kumar followed up with you last time regarding your hypertension, osteoarthritis, osteoporosis, hypothyroidism, allergic rhinitis and kidney stones.  Have you noticed any changes or do you have any concerns regarding these issues?'],\n",
       " ['Patient: No.',\n",
       "  'Doctor: Have you had any fever or chills, cough, congestion, nausea, vomiting, chest pain, chest pressure?',\n",
       "  'Patient: No.'],\n",
       " ['Doctor: Great. Also, for our records, how old are you and what race do you identify yourself as?',\n",
       "  'Patient: I am seventy six years old and identify as a white female.']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_dialogues(turn_split(dataset_split['dialogue'][0]),n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialog sentence:  Doctor: What brings you back into the clinic today, miss? \n",
      "Patient: I came in for a refill of my blood pressure medicine. \n",
      "Doctor: It looks like Doctor Kumar followed up with you last time regarding your hypertension, osteoarthritis, osteoporosis, hypoth\n",
      "Summary sentence:  The patient is a 76-year-old white female who presents to the clinic today originally for hypertension and a med check.\n",
      "Dialog sentence:  itis, osteoporosis, hypothyroidism, allergic rhinitis and kidney stones.  Have you noticed any changes or do you have any concerns regarding these issues?  \n",
      "Patient: No. \n",
      "Doctor: Have you had any fever or chills, cough, congestion, nausea, vomiting, ches\n",
      "Summary sentence:  She has a history of hypertension, osteoarthritis, osteoporosis, hypothyroidism, allergic rhinitis and kidney stones.\n",
      "Dialog sentence:  on, nausea, vomiting, chest pain, chest pressure?\n",
      "Patient: No.  \n",
      "Doctor: Great. Also, for our records, how old are you and what race do you identify yourself as?\n",
      "Patient: I am seventy six years old and identify as a white female.\n",
      "Summary sentence:  The patient is a 76-year-old white female who presents to the clinic today originally for hypertension and a med check.\n"
     ]
    }
   ],
   "source": [
    "dataset_split = pd.read_csv(\"./MEDIQA-Chat-Training-ValidationSets-Feb-10-2023/TaskA/TaskA-TrainingSet.csv\")\n",
    "summary=dataset_split['section_text'][0]\n",
    "text = dataset_split['dialogue'][0]\n",
    "utterences_summary_alighment(summary=summary,dialogue=text,MAX_LEN=256,OVERLAP=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dia2note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31c27fd85116cead093449143a69b82bfd3a5d8b2c8cf1f31800822541abbb48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
